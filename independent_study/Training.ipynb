{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784208b9",
   "metadata": {},
   "source": [
    "# Custom Training Code <br>\n",
    "Built by Alex Fisher and Kevin Parra-Olmedo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c20eae",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# activate GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "tf.config.set_soft_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pretty_midi\n",
    "import librosa\n",
    "\n",
    "\n",
    "from basic_pitch import inference\n",
    "from basic_pitch import models\n",
    "\n",
    "from basic_pitch.constants import (\n",
    "    ANNOT_N_FRAMES,\n",
    "    ANNOTATIONS_FPS,\n",
    "    ANNOTATIONS_N_SEMITONES,\n",
    "    AUDIO_N_SAMPLES,\n",
    "    N_FREQ_BINS_CONTOURS,\n",
    "    AUDIO_SAMPLE_RATE,\n",
    "    FFT_HOP\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "SPLIT_INTERVAL = 2\n",
    "DATASET_PERCENTAGE = 1\n",
    "\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d175e",
   "metadata": {},
   "source": [
    "## Load in sample dataset files<br>\n",
    "We are using a small sample from MAESTRO dataset's 100 GB of midi/wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('./datasets/maestro-v3.0.0/maestro-v3.0.0.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Number of samples:\", len(data['midi_filename']))\n",
    "\n",
    "audio_midi_pairs = []\n",
    "for i in range(0, len(data['midi_filename'])):\n",
    "    audio_filename = './datasets/maestro-v3.0.0/' + data['audio_filename'][f\"{i}\"]\n",
    "    midi_filename = './datasets/maestro-v3.0.0/' + data['midi_filename'][f\"{i}\"]\n",
    "    audio_midi_pairs.append((audio_filename, midi_filename))\n",
    "\n",
    "audio_midi_pairs = audio_midi_pairs[:int(len(audio_midi_pairs) * 0.01)]\n",
    "print(\"Number of samples used: \" + str(len(audio_midi_pairs)))\n",
    "print(audio_midi_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c586f7d",
   "metadata": {},
   "source": [
    "# Preprocess audio and MIDI pair files\n",
    "Audio needs to fit what model takes as input (windowed audio, uses Basic Pitch's inference get_audio_input function)<br>\n",
    "MIDI needs to match what model outputs (binary matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2070e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO PREPROCESS MIDI TO BINARY CLASSIFICATION NOTE ONSET MATRIX\n",
    "def midi_to_piano_onset_matrix(midi_path, frames_per_second=ANNOTATIONS_FPS):\n",
    "    \"\"\"\n",
    "    Convert MIDI file to a binary matrix representing onset of piano keys using a set FPS.\n",
    "\n",
    "    Parameters:\n",
    "    - midi_path (str): Path to the MIDI file.\n",
    "    - frames_per_second (int): Number of frames per second for the binary representation.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Binary matrix where rows represent the 88 piano keys and columns are time frames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "\n",
    "    # Duration of the MIDI file in seconds\n",
    "    duration = midi_data.get_end_time()\n",
    "\n",
    "    # 88 keys for standard piano\n",
    "    num_piano_keys = 88\n",
    "\n",
    "    # Calculate the total number of frames based on the FPS\n",
    "    total_frames = int(duration * frames_per_second)\n",
    "\n",
    "    # Initialize binary matrix with zeros\n",
    "    binary_matrix = np.zeros((total_frames, num_piano_keys))\n",
    "\n",
    "    for instrument in midi_data.instruments:\n",
    "        for note in instrument.notes:\n",
    "            # Only consider valid piano notes (from 21 to 108)\n",
    "            if 21 <= note.pitch <= 108:\n",
    "                # Find the frame for this onset time\n",
    "                onset_frame = int(note.start * frames_per_second)\n",
    "\n",
    "                # Prevent indexing beyond the matrix size\n",
    "                if onset_frame < total_frames:\n",
    "                    # Adjust the pitch value to fit within our matrix's row indices (0-87)\n",
    "                    adjusted_pitch = note.pitch - 21\n",
    "\n",
    "                    # Mark the onset in the binary matrix\n",
    "                    binary_matrix[onset_frame, adjusted_pitch] = 1\n",
    "\n",
    "    return binary_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598137ca",
   "metadata": {},
   "source": [
    "The following three data processing methods produce functional processed data for model training. However, only method 3 is considered scalable for training on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1: PREPROCESS ENTIRE DATASET AND STORE IN MEMORY\n",
    "# THIS USES THE MOST MEMORY, UNSCALABLE SOLUTION\n",
    "\n",
    "os_x = []\n",
    "os_y = []\n",
    "\n",
    "\n",
    "for idx, (audio_filename, midi_filename) in enumerate(audio_midi_pairs[:int(len(audio_midi_pairs)*0.2)]):\n",
    "    offset = 0\n",
    "    # preprocess midi\n",
    "    onsets = midi_to_piano_onset_matrix(midi_filename, frames_per_second=ANNOTATIONS_FPS)\n",
    "    while offset < librosa.get_duration(filename=audio_filename) - SPLIT_INTERVAL:\n",
    "        # preprocess audio\n",
    "\n",
    "        n_overlapping_frames = 30\n",
    "        overlap_len = n_overlapping_frames * FFT_HOP\n",
    "        hop_size = AUDIO_N_SAMPLES - overlap_len\n",
    "\n",
    "        # modified get_input_audio function to get audio from offset\n",
    "        assert overlap_len % 2 == 0, \"overlap_length must be even, got {}\".format(overlap_len)\n",
    "        audio_original, _ = librosa.load(audio_filename, sr=AUDIO_SAMPLE_RATE, offset=offset, duration=SPLIT_INTERVAL, mono=True)\n",
    "\n",
    "        original_length = audio_original.shape[0]\n",
    "        audio_original = np.concatenate([np.zeros((int(overlap_len / 2),), dtype=np.float32), audio_original])\n",
    "        audio_windowed, window_times = inference.window_audio_file(audio_original, hop_size)\n",
    "    \n",
    "        os_x.append(audio_windowed)\n",
    "\n",
    "        split_onsets = onsets[int(offset*ANNOTATIONS_FPS):int((offset+SPLIT_INTERVAL)*ANNOTATIONS_FPS), :]\n",
    "        if (split_onsets.shape[0] < ANNOTATIONS_FPS * SPLIT_INTERVAL):\n",
    "            padding = ANNOTATIONS_FPS * SPLIT_INTERVAL - split_onsets.shape[0]\n",
    "            split_onsets = np.pad(split_onsets, [(0, padding), (0, 0)], 'constant')\n",
    "        os_y.append(split_onsets)\n",
    "\n",
    "        offset += SPLIT_INTERVAL\n",
    "\n",
    "tensor_dataset = tf.data.Dataset.from_tensor_slices((os_x, os_y))\n",
    "train_dataset = tensor_dataset.take(int(len(tensor_dataset)*0.8))\n",
    "val_dataset = tensor_dataset.skip(int(len(tensor_dataset)*0.8))\n",
    "\n",
    "print(\"train_dataset: \", train_dataset)\n",
    "print(\"val_dataset: \", val_dataset)\n",
    "\n",
    "take_count = sum(1 for _ in train_dataset)\n",
    "print(f\"Size of take_dataset: {take_count}\")\n",
    "\n",
    "skip_count = sum(1 for _ in val_dataset)\n",
    "print(f\"Size of skip_dataset: {skip_count}\")\n",
    "\n",
    "ds_count = sum(1 for _ in tensor_dataset)\n",
    "print(f\"Size of batched_dataset: {ds_count}\")\n",
    "\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "for audio, onset in train_dataset.take(1):  # Adjust the number taken as needed\n",
    "    print(\"Audio shape:\", audio.shape)\n",
    "    print(\"Onset shape:\", onset.shape)\n",
    "    # Optionally, visually inspect the actual data\n",
    "    print(\"Audio data sample:\", audio[0])  # Inspect first sample of the batch\n",
    "    print(\"Onset data sample:\", onset[0])  # Inspect first sample of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 2: PROCESS ENTIRE DATA BEFOREHAND AND SAVE IN TFRECORDS\n",
    "# THIS IS ALSO UNSCALABLE, SAVES ENTIRE PROCESSED DATASET IN PERSISTENT MEMORY AS A BINARY FILE\n",
    "\n",
    "def serialize_example(audio_windowed, split_onsets):\n",
    "    # Flatten the tensors using TensorFlow's reshape function\n",
    "    audio_windowed_flat = tf.reshape(audio_windowed, [-1])\n",
    "    split_onsets_flat = tf.reshape(split_onsets, [-1])\n",
    "\n",
    "    # Create a feature dictionary\n",
    "    feature = {\n",
    "        'audio_windowed': tf.train.Feature(float_list=tf.train.FloatList(value=audio_windowed_flat.numpy())),\n",
    "        'split_onsets': tf.train.Feature(float_list=tf.train.FloatList(value=split_onsets_flat.numpy()))\n",
    "    }\n",
    "    # Create an Example\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "with tf.io.TFRecordWriter('train.tfrecords') as writer:\n",
    "    for idx, (audio_filename, midi_filename) in enumerate(audio_midi_pairs[:int(len(audio_midi_pairs)*1)]):\n",
    "        offset = 0\n",
    "        # preprocess midi\n",
    "        onsets = midi_to_piano_onset_matrix(midi_filename, frames_per_second=ANNOTATIONS_FPS)\n",
    "        while offset < librosa.get_duration(filename=audio_filename) - SPLIT_INTERVAL:\n",
    "            # preprocess audio\n",
    "\n",
    "            n_overlapping_frames = 30\n",
    "            overlap_len = n_overlapping_frames * FFT_HOP\n",
    "            hop_size = AUDIO_N_SAMPLES - overlap_len\n",
    "\n",
    "            # modified get_input_audio function to get audio from offset\n",
    "            assert overlap_len % 2 == 0, \"overlap_length must be even, got {}\".format(overlap_len)\n",
    "            audio_original, _ = librosa.load(audio_filename, sr=AUDIO_SAMPLE_RATE, offset=offset, duration=SPLIT_INTERVAL, mono=True)\n",
    "\n",
    "            original_length = audio_original.shape[0]\n",
    "            audio_original = np.concatenate([np.zeros((int(overlap_len / 2),), dtype=np.float32), audio_original])\n",
    "            audio_windowed, window_times = inference.window_audio_file(audio_original, hop_size)\n",
    "\n",
    "            split_onsets = onsets[int(offset*ANNOTATIONS_FPS):int((offset+SPLIT_INTERVAL)*ANNOTATIONS_FPS), :]\n",
    "            if (split_onsets.shape[0] < ANNOTATIONS_FPS * SPLIT_INTERVAL):\n",
    "                padding = ANNOTATIONS_FPS * SPLIT_INTERVAL - split_onsets.shape[0]\n",
    "                split_onsets = np.pad(split_onsets, [(0, padding), (0, 0)], 'constant')\n",
    "            \n",
    "            # write sample to file\n",
    "            sample = serialize_example(audio_windowed, split_onsets)\n",
    "            writer.write(sample)\n",
    "\n",
    "            offset += SPLIT_INTERVAL\n",
    "\n",
    "\n",
    "def parse_tfrecord(example_proto, audio_shape, onsets_shape):\n",
    "    # Define the features to be extracted\n",
    "    features = {\n",
    "        'audio_windowed': tf.io.FixedLenFeature([np.prod(audio_shape)], tf.float32),\n",
    "        'split_onsets': tf.io.FixedLenFeature([np.prod(onsets_shape)], tf.float32),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, features)\n",
    "\n",
    "    # Reshape the data to its original shape\n",
    "    audio_windowed = tf.reshape(parsed_features['audio_windowed'], audio_shape)\n",
    "    split_onsets = tf.reshape(parsed_features['split_onsets'], onsets_shape)\n",
    "    \n",
    "    return audio_windowed, split_onsets\n",
    "\n",
    "# Replace these with the actual shapes of your audio_windowed and split_onsets\n",
    "audio_shape = (2, 43844, 1) \n",
    "onsets_shape = (172, 88)  \n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset('train.tfrecords')\n",
    "parsed_dataset = raw_dataset.map(lambda x: parse_tfrecord(x, audio_shape, onsets_shape))\n",
    "\n",
    "dataset_size = len(list(parsed_dataset))  # Number of items in the dataset\n",
    "\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset = parsed_dataset.take(train_size)\n",
    "val_dataset = parsed_dataset.skip(train_size)\n",
    "\n",
    "buffer_size = dataset_size  # Set buffer size to the dataset size for complete shuffling\n",
    "\n",
    "#train_dataset = train_dataset.shuffle(buffer_size, reshuffle_each_iteration=True)\n",
    "#val_dataset = val_dataset.shuffle(buffer_size, reshuffle_each_iteration=True)\n",
    "\n",
    "\n",
    "for audio, onset in train_dataset.take(1):  # Adjust the number taken as needed\n",
    "    print(\"Audio shape:\", audio.shape)\n",
    "    print(\"Onset shape:\", onset.shape)\n",
    "    # Optionally, visually inspect the actual data\n",
    "    print(\"Audio data sample:\", audio[0])  # Inspect first sample of the batch\n",
    "    print(\"Onset data sample:\", onset[0])  # Inspect first sample of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 3: PROCESS DATA ON-THE-FLY USING TENSORFLOW DATA GENERATOR\n",
    "# THIS METHOD IS SCALABLE, IT WILL PROCESS THE DATASET SEQUENTIALLY AS DATA IS PULLED, NO MATTER HOW LARGE THE DATASET IS\n",
    "\n",
    "def preprocess_data(audio_filename, midi_filename, AUDIO_SAMPLE_RATE, SPLIT_INTERVAL, ANNOTATIONS_FPS, AUDIO_N_SAMPLES, FFT_HOP):\n",
    "    # Initialize lists to store the preprocessed data\n",
    "\n",
    "    n_overlapping_frames = 30\n",
    "    overlap_len = n_overlapping_frames * FFT_HOP\n",
    "    hop_size = AUDIO_N_SAMPLES - overlap_len\n",
    "    offset = 0\n",
    "    onsets = midi_to_piano_onset_matrix(midi_filename, frames_per_second=ANNOTATIONS_FPS)\n",
    "\n",
    "    while offset < librosa.get_duration(path=audio_filename) - SPLIT_INTERVAL:\n",
    "        audio_original, _ = librosa.load(audio_filename, sr=AUDIO_SAMPLE_RATE, offset=offset, duration=SPLIT_INTERVAL, mono=True)\n",
    "\n",
    "        audio_original = np.concatenate([np.zeros((int(overlap_len / 2),), dtype=np.float32), audio_original])\n",
    "        audio_windowed, window_times = inference.window_audio_file(audio_original, hop_size)\n",
    "\n",
    "        split_onsets = onsets[int(offset*ANNOTATIONS_FPS):int((offset+SPLIT_INTERVAL)*ANNOTATIONS_FPS), :]\n",
    "        if (split_onsets.shape[0] < ANNOTATIONS_FPS * SPLIT_INTERVAL):\n",
    "            padding = ANNOTATIONS_FPS * SPLIT_INTERVAL - split_onsets.shape[0]\n",
    "            split_onsets = np.pad(split_onsets, [(0, padding), (0, 0)], 'constant')\n",
    "\n",
    "        yield np.array(audio_windowed), np.array(split_onsets)\n",
    "        offset += SPLIT_INTERVAL\n",
    "\n",
    "\n",
    "# NEED TO INITIALIZE DATA GENERATOR TO SAVE MEMORY (DATA WILL BE LOADED AS THE MODEL NEEDS IT, NOT ALL BEFORE TRAINING)\n",
    "\n",
    "# create data generator\n",
    "def data_generator(dataset, AUDIO_SAMPLE_RATE, SPLIT_INTERVAL, ANNOTATIONS_FPS, AUDIO_N_SAMPLES, FFT_HOP):\n",
    "    print(\"CALLED DATA GENERATOR\")\n",
    "    for audio_filename, midi_filename in dataset:\n",
    "        n_overlapping_frames = 30\n",
    "        overlap_len = n_overlapping_frames * FFT_HOP\n",
    "        hop_size = AUDIO_N_SAMPLES - overlap_len\n",
    "        offset = 0\n",
    "        onsets = midi_to_piano_onset_matrix(midi_filename, frames_per_second=ANNOTATIONS_FPS)\n",
    "\n",
    "        while offset < librosa.get_duration(filename=audio_filename) - SPLIT_INTERVAL:\n",
    "            audio_original, _ = librosa.load(audio_filename, sr=AUDIO_SAMPLE_RATE, offset=offset, duration=SPLIT_INTERVAL, mono=True)\n",
    "\n",
    "            audio_original = np.concatenate([np.zeros((int(overlap_len / 2),), dtype=np.float32), audio_original])\n",
    "            audio_windowed, window_times = inference.window_audio_file(audio_original, hop_size)\n",
    "\n",
    "            split_onsets = onsets[int(offset*ANNOTATIONS_FPS):int((offset+SPLIT_INTERVAL)*ANNOTATIONS_FPS), :]\n",
    "            if (split_onsets.shape[0] < ANNOTATIONS_FPS * SPLIT_INTERVAL):\n",
    "                padding = ANNOTATIONS_FPS * SPLIT_INTERVAL - split_onsets.shape[0]\n",
    "                split_onsets = np.pad(split_onsets, [(0, padding), (0, 0)], 'constant')\n",
    "\n",
    "            yield np.array(audio_windowed), np.array(split_onsets)\n",
    "            offset += SPLIT_INTERVAL\n",
    "\n",
    "output_types = (tf.float32, tf.float64)  # Modify as per your data types\n",
    "output_shapes = (tf.TensorShape([SPLIT_INTERVAL, 43844, 1]), tf.TensorShape([172, 88]))  # Modify as per your data shapes\n",
    "\n",
    "# Example of splitting the dataset\n",
    "train_size = int(0.8 * len(audio_midi_pairs))\n",
    "train_audio_midi_pairs = audio_midi_pairs[:train_size]\n",
    "val_audio_midi_pairs = audio_midi_pairs[train_size:]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_audio_midi_pairs, AUDIO_SAMPLE_RATE, SPLIT_INTERVAL, ANNOTATIONS_FPS, AUDIO_N_SAMPLES, FFT_HOP),\n",
    "    output_types=output_types,\n",
    "    output_shapes=output_shapes\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_audio_midi_pairs, AUDIO_SAMPLE_RATE, SPLIT_INTERVAL, ANNOTATIONS_FPS, AUDIO_N_SAMPLES, FFT_HOP),\n",
    "    output_types=output_types,\n",
    "    output_shapes=output_shapes\n",
    ")\n",
    "\n",
    "batch_size = BATCH_SIZE  # Adjust according to your needs\n",
    "\n",
    "\n",
    "for audio, onset in train_dataset.take(1):  # Adjust the number taken as needed\n",
    "    print(\"Audio shape:\", audio.shape)\n",
    "    print(\"Onset shape:\", onset.shape)\n",
    "    # Optionally, visually inspect the actual data\n",
    "    print(\"Audio data sample:\", audio[0])  # Inspect first sample of the batch\n",
    "    print(\"Onset data sample:\", onset[0])  # Inspect first sample of the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110ce8d",
   "metadata": {},
   "source": [
    "## Train the model using the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT OUT ALL TRAINABLE LAYERS OF THE MODEL\n",
    "# Iterate through the layers and print the layer name and its trainable status\n",
    "for layer in models.model().layers:\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"Trainable: {layer.trainable}\")\n",
    "    for weight in layer.trainable_weights:\n",
    "        print(f\"\\tWeight: {weight.name}, Shape: {weight.shape}\")\n",
    "\n",
    "# If you only want to see layers with trainable weights:\n",
    "print(\"\\nOnly layers with trainable weights:\")\n",
    "for layer in models.model().layers:\n",
    "    if layer.trainable_weights:\n",
    "        print(f\"Layer: {layer.name}\")\n",
    "        for weight in layer.trainable_weights:\n",
    "            print(f\"\\tWeight: {weight.name}, Shape: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab325f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE CUSTOM LOSS FUNCTION FOR WEIGHTED BINARY CROSS ENTROPY\n",
    "class WeightedBinaryCrossEntropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, pos_weight, neg_weight, from_logits=False, name='weighted_binary_crossentropy'):\n",
    "        super().__init__(name=name)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.neg_weight = neg_weight\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if not self.from_logits:\n",
    "            #print(\"\\ny_pred: \", y_pred)\n",
    "            original_length = 22050 * SPLIT_INTERVAL\n",
    "            n_overlapping_frames = 30\n",
    "            unwrapped_y_pred = self.unwrap_output_custom(y_pred, original_length, n_overlapping_frames)\n",
    "\n",
    "            # Manually calculate the weighted binary cross-entropy for predictions that aren't logits\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            unwrapped_y_pred = tf.clip_by_value(unwrapped_y_pred, epsilon, 1.0 - epsilon)\n",
    "\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            unwrapped_y_pred = tf.cast(unwrapped_y_pred, tf.float32)\n",
    "            pos_weight = tf.cast(self.pos_weight, tf.float32)\n",
    "            neg_weight = tf.cast(self.neg_weight, tf.float32)\n",
    "\n",
    "            loss = -y_true * tf.math.log(unwrapped_y_pred) * pos_weight - (1.0 - y_true) * tf.math.log(1.0 - unwrapped_y_pred) * neg_weight\n",
    "        else:\n",
    "            # Use TensorFlow's built-in function for logits\n",
    "            loss = tf.nn.weighted_cross_entropy_with_logits(labels=y_true, logits=y_pred, pos_weight=self.pos_weight)\n",
    "\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    # custom unwrap output function that remains compatible with TensorFlow's graph execution\n",
    "    def unwrap_output_custom(self, output: tf.Tensor, audio_original_length: int, n_overlapping_frames: int) -> tf.Tensor:\n",
    "        \"\"\"Unwrap batched model predictions to a single matrix.\n",
    "\n",
    "        Args:\n",
    "            output: tensor (n_batches, n_times_short, n_freqs)\n",
    "            audio_original_length: length of original audio signal (in samples)\n",
    "            n_overlapping_frames: number of overlapping frames in the output\n",
    "\n",
    "        Returns:\n",
    "            tensor (n_times, n_freqs)\n",
    "        \"\"\"\n",
    "        output_rank = tf.rank(output)\n",
    "        #print(\"output_rank: \", output_rank)\n",
    "        \n",
    "        def process_output():\n",
    "            n_olap = int(0.5 * n_overlapping_frames)\n",
    "            if n_olap > 0:\n",
    "                output_processed = output[:, n_olap:-n_olap, :]\n",
    "            else:\n",
    "                output_processed = output\n",
    "                \n",
    "            output_shape = tf.shape(output_processed)\n",
    "            n_output_frames_original = tf.cast(tf.floor(audio_original_length * (ANNOTATIONS_FPS / AUDIO_SAMPLE_RATE)), tf.int32)\n",
    "            unwrapped_output = tf.reshape(output_processed, [output_shape[0] * output_shape[1], output_shape[2]])\n",
    "            return unwrapped_output[:n_output_frames_original, :]  # trim to original audio length\n",
    "        \n",
    "        def handle_invalid_rank():\n",
    "            # Print a warning message and return a dummy tensor\n",
    "            tf.print(f\"Warning: Expected output rank to be 3, got {output_rank}\")\n",
    "            return tf.zeros((0, 0), dtype=output.dtype)\n",
    "\n",
    "        return tf.cond(tf.equal(output_rank, 3), process_output, handle_invalid_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16302a",
   "metadata": {},
   "source": [
    "## Try built-in tensorflow train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "\n",
    "#model_train = models.model()\n",
    "model_train = \n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "onset_loss_function = WeightedBinaryCrossEntropy(pos_weight=0.99, neg_weight=0.01)\n",
    "contour_loss_function = WeightedBinaryCrossEntropy(pos_weight=0.99, neg_weight=0.01)\n",
    "note_loss_function = WeightedBinaryCrossEntropy(pos_weight=0.99, neg_weight=0.01)\n",
    "model_train.compile(optimizer=adam_optimizer, loss={\"onset\": onset_loss_function, \"note\": note_loss_function})\n",
    "\n",
    "# train model\n",
    "num_epochs = 1\n",
    "\n",
    "model_train.fit(train_dataset, validation_data=val_dataset, epochs=num_epochs, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3496f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our trained version of the model\n",
    "\n",
    "model_train.save('saved_models/dec04_train_99posweight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac43be2",
   "metadata": {},
   "source": [
    "## Test and Evaluate Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command line execution to output resulting midi from our trained model\n",
    "!python ../basic_pitch_original/basic_pitch/predict.py --model_path \"saved_models/dec04_train_99posweight_onthefly\" \"model_predictions/our_model/dec04_train_99posweight_onthefly/\" \"model_predictions/_test_audio/MIDI-Unprocessed_Recital1-3_MID--AUDIO_03_R1_2018_wav--5.wav\"\n",
    "\n",
    "# command line execution to output resulting midi from spotify model for comparison\n",
    "!python ../basic_pitch_original/basic_pitch/predict.py \"model_predictions/spotify_model/\" \"model_predictions/_test_audio/MIDI-Unprocessed_Recital1-3_MID--AUDIO_03_R1_2018_wav--5.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497187f8",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation\n",
    "##### The following code is incomplete. Model Evaluation was done primarily through qualitative analysis (listening to the MIDI by ear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO EVALUATE MODEL\n",
    "import mir_eval\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(data, model, threshold=0.5):\n",
    "    # Lists to hold ground truth and predictions for evaluation\n",
    "    reference_notes = []\n",
    "    estimated_notes = []\n",
    "\n",
    "    # Iterate over your dataset\n",
    "    for audio_file, midi_file in data:\n",
    "        # Predict the piano roll with your model\n",
    "        _, y_pred_midi = inference.predict(audio, model)\n",
    "        print(\"MODEL OUTPUT:\\n\", \"\\n\\nData:\\n\", y_pred_midi)\n",
    "\n",
    "    # Compute metrics using mir_eval\n",
    "    scores = {\n",
    "        'F-measure': [],\n",
    "        'F-measure-no-offset': [],\n",
    "        'Frame-level Accuracy': []\n",
    "    }\n",
    "    for ref, est in zip(reference_notes, estimated_notes):\n",
    "        # mir_eval requires specific formats for reference and estimated notes\n",
    "        ref_intervals, ref_pitches = mir_eval.util.piano_roll_to_intervals(ref)\n",
    "        est_intervals, est_pitches = mir_eval.util.piano_roll_to_intervals(est)\n",
    "\n",
    "        # Evaluate\n",
    "        p, r, f_measure, _ = mir_eval.transcription.precision_recall_f1_overlap(ref_intervals, ref_pitches, est_intervals, est_pitches)\n",
    "        scores['F-measure'].append(f_measure)\n",
    "\n",
    "        # Compute F_no\n",
    "        f_no = mir_eval.transcription.f_measure_without_offset(ref_intervals, ref_pitches, est_intervals, est_pitches)\n",
    "        scores['F-measure-no-offset'].append(f_no)\n",
    "\n",
    "        # Compute frame-level accuracy\n",
    "        acc = mir_eval.transcription_accuracy(ref_intervals, ref_pitches, est_intervals, est_pitches)\n",
    "        scores['Frame-level Accuracy'].append(acc)\n",
    "\n",
    "    # Average the scores\n",
    "    for key in scores:\n",
    "        scores[key] = np.mean(scores[key])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE EVALUATION DATA\n",
    "audio_filename = \"model_predictions/_test_audio/MIDI-Unprocessed_Recital1-3_MID--AUDIO_03_R1_2018_wav--5.wav\"\n",
    "midi_filename = \"model_predictions/_test_audio/MIDI-Unprocessed_Recital1-3_MID--AUDIO_03_R1_2018_wav--5.midi\"\n",
    "\n",
    "eval_audio_midi_pairs = [(audio_filename, midi_filename)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# You need to prepare 'validation_data' in the format that your model expects\n",
    "\n",
    "scores = evaluate_model(eval_audio_midi_pairs, model_nov26_02)\n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
