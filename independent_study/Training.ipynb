{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eba1832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pretty_midi\n",
    "import librosa\n",
    "\n",
    "\n",
    "from basic_pitch import inference\n",
    "from basic_pitch import models\n",
    "\n",
    "from basic_pitch.constants import (\n",
    "    ANNOT_N_FRAMES,\n",
    "    ANNOTATIONS_FPS,\n",
    "    ANNOTATIONS_N_SEMITONES,\n",
    "    AUDIO_N_SAMPLES,\n",
    "    N_FREQ_BINS_CONTOURS,\n",
    "    AUDIO_SAMPLE_RATE,\n",
    "    FFT_HOP\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8e0af8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the ground truth MIDI files\n",
    "# glob is a pattern matching utility for files\n",
    "\n",
    "#use maestro-v3.0.0.json to get needed files\n",
    "\n",
    "with open('datasets/maestro/maestro-v3.0.0/maestro-v3.0.0.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    midi_filenames = data['midi_filename']\n",
    "    audio_filenames = data['audio_filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8bd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spotify basic pitch model\n",
    "model = models.model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1a8efcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\_AlexFiles\\Coding\\Python\\FALL2023_IndependentStudy\\audio_to_midi_vst\\independent_study\\Training.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/_AlexFiles/Coding/Python/FALL2023_IndependentStudy/audio_to_midi_vst/independent_study/Training.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(x_train[\u001b[39m1\u001b[39;49m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "y = model.predict(x_train[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9daaf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mir_eval' has no attribute 'midi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\_AlexFiles\\Coding\\Python\\FALL2023_IndependentStudy\\audio_to_midi_vst\\independent_study\\Training.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/_AlexFiles/Coding/Python/FALL2023_IndependentStudy/audio_to_midi_vst/independent_study/Training.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Compare the two sets of MIDI files using mir_eval\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/_AlexFiles/Coding/Python/FALL2023_IndependentStudy/audio_to_midi_vst/independent_study/Training.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m scores \u001b[39m=\u001b[39m mir_eval\u001b[39m.\u001b[39;49mmidi\u001b[39m.\u001b[39mevaluate(gt_midi, model_midi)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mir_eval' has no attribute 'midi'"
     ]
    }
   ],
   "source": [
    "# Compare the two sets of MIDI files using mir_eval\n",
    "scores = mir_eval.midi.evaluate(gt_midi, model_midi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d1e6991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 4s 273ms/step\n",
      "15/15 [==============================] - 4s 270ms/step\n",
      "9/9 [==============================] - 2s 273ms/step\n",
      "17/17 [==============================] - 5s 273ms/step\n",
      "8/8 [==============================] - 2s 267ms/step\n",
      "8/8 [==============================] - 2s 264ms/step\n",
      "4/4 [==============================] - 1s 211ms/step\n",
      "11/11 [==============================] - 3s 275ms/step\n",
      "3/3 [==============================] - 1s 248ms/step\n",
      "4/4 [==============================] - 1s 222ms/step\n"
     ]
    }
   ],
   "source": [
    "# preprocess audio\n",
    "x_train = []\n",
    "y_pred = []\n",
    "\n",
    "for idx in range(0, 10):\n",
    "\n",
    "    audio_file = \"datasets/maestro/maestro-v3.0.0/\" + audio_filenames['{}'.format(idx)]\n",
    "\n",
    "    # overlap 30 frames\n",
    "    n_overlapping_frames = 30\n",
    "    overlap_len = n_overlapping_frames * FFT_HOP\n",
    "    hop_size = AUDIO_N_SAMPLES - overlap_len\n",
    "    audio_windowed, _, audio_original_length = inference.get_audio_input(audio_file, overlap_len, hop_size)\n",
    "\n",
    "    x_train.append(audio_windowed)\n",
    "    output = model.predict(audio_windowed)\n",
    "    unwrapped_output = {k: inference.unwrap_output(output[k], audio_original_length, n_overlapping_frames) for k in output}\n",
    "    y_pred.append(unwrapped_output)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2070e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_piano_onset_matrix(midi_path, frames_per_second=ANNOTATIONS_FPS):\n",
    "    \"\"\"\n",
    "    Convert MIDI file to a binary matrix representing onset of piano keys using a set FPS.\n",
    "\n",
    "    Parameters:\n",
    "    - midi_path (str): Path to the MIDI file.\n",
    "    - frames_per_second (int): Number of frames per second for the binary representation.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Binary matrix where rows represent the 88 piano keys and columns are time frames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
    "\n",
    "    # Duration of the MIDI file in seconds\n",
    "    duration = midi_data.get_end_time()\n",
    "\n",
    "    # 88 keys for standard piano\n",
    "    num_piano_keys = 88\n",
    "\n",
    "    # Calculate the total number of frames based on the FPS\n",
    "    total_frames = int(duration * frames_per_second)\n",
    "\n",
    "    # Initialize binary matrix with zeros\n",
    "    binary_matrix = np.zeros((total_frames, num_piano_keys))\n",
    "\n",
    "    for instrument in midi_data.instruments:\n",
    "        for note in instrument.notes:\n",
    "            # Only consider valid piano notes (from 21 to 108)\n",
    "            if 21 <= note.pitch <= 108:\n",
    "                # Find the frame for this onset time\n",
    "                onset_frame = int(note.start * frames_per_second)\n",
    "\n",
    "                # Prevent indexing beyond the matrix size\n",
    "                if onset_frame < total_frames:\n",
    "                    # Adjust the pitch value to fit within our matrix's row indices (0-87)\n",
    "                    adjusted_pitch = note.pitch - 21\n",
    "\n",
    "                    # Mark the onset in the binary matrix\n",
    "                    binary_matrix[onset_frame, adjusted_pitch] = 1\n",
    "\n",
    "    return binary_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9aef2577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train MIDI to binary onset matrix shape:  (60567, 88)\n",
      "Y_prediction from model shape:  (60567, 88)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "midi_file = \"datasets/maestro/maestro-v3.0.0/\" + midi_filenames['{}'.format(idx)]\n",
    "\n",
    "pm_midi = pretty_midi.PrettyMIDI(midi_file)\n",
    "\n",
    "duration = pm_midi.get_end_time()\n",
    "\n",
    "onsets = midi_to_piano_onset_matrix(midi_file, frames_per_second=ANNOTATIONS_FPS)\n",
    "padding = y_pred[idx]['onset'].shape[0] - onsets.shape[0]\n",
    "onsets = np.pad(onsets, [(0, padding), (0, 0)], 'constant')\n",
    "\n",
    "print(\"Y_train MIDI to binary onset matrix shape: \", onsets.shape)\n",
    "print(\"Y_prediction from model shape: \", y_pred[idx]['onset'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3778e277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 4s 268ms/step\n",
      "15/15 [==============================] - 4s 294ms/step\n",
      "9/9 [==============================] - 3s 299ms/step\n",
      "17/17 [==============================] - 5s 298ms/step\n",
      "8/8 [==============================] - 2s 293ms/step\n",
      "8/8 [==============================] - 2s 288ms/step\n",
      "4/4 [==============================] - 1s 230ms/step\n",
      "11/11 [==============================] - 3s 294ms/step\n",
      "3/3 [==============================] - 1s 273ms/step\n",
      "4/4 [==============================] - 1s 239ms/step\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "y_pred = []\n",
    "\n",
    "for idx in range(0, 10):\n",
    "\n",
    "    # preprocess audio\n",
    "    audio_file = \"datasets/maestro/maestro-v3.0.0/\" + audio_filenames['{}'.format(idx)]\n",
    "\n",
    "    n_overlapping_frames = 30\n",
    "    overlap_len = n_overlapping_frames * FFT_HOP\n",
    "    hop_size = AUDIO_N_SAMPLES - overlap_len\n",
    "    audio_windowed, _, audio_original_length = inference.get_audio_input(audio_file, overlap_len, hop_size)\n",
    "\n",
    "    x_train.append(audio_windowed)\n",
    "    output = model.predict(audio_windowed)\n",
    "    unwrapped_output = {k: inference.unwrap_output(output[k], audio_original_length, n_overlapping_frames) for k in output}\n",
    "    y_pred.append(unwrapped_output)\n",
    "\n",
    "\n",
    "    # preprocess midi\n",
    "    midi_file = \"datasets/maestro/maestro-v3.0.0/\" + midi_filenames['{}'.format(idx)]\n",
    "\n",
    "    pm_midi = pretty_midi.PrettyMIDI(midi_file)\n",
    "    onsets = midi_to_piano_onset_matrix(midi_file, frames_per_second=ANNOTATIONS_FPS)\n",
    "    padding = y_pred[idx]['onset'].shape[0] - onsets.shape[0]\n",
    "    onsets = np.pad(onsets, [(0, padding), (0, 0)], 'constant')\n",
    "    y_train.append(onsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6a7cd203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85.99908121847122, 85.99975048108587, 85.9984533224385, 85.99935673157775, 85.99752409290758, 85.99765904452198, 85.99927325581396, 85.99889281996315, 85.99592687252594, 85.99768746472216]\n"
     ]
    }
   ],
   "source": [
    "frames_ratio = []\n",
    "for idx in range(0,10):\n",
    "    y_pred[idx]['onset'].shape\n",
    "    filename = \"datasets/maestro/maestro-v3.0.0/\" + audio_filenames['{}'.format(idx)]\n",
    "    audio, sr = librosa.load(filename, sr=AUDIO_SAMPLE_RATE)\n",
    "    # get file length in seconds\n",
    "    file_length = librosa.get_duration(y = audio, sr = sr)\n",
    "    # get frames / duration\n",
    "    frames_ratio.append(y_pred[idx]['onset'].shape[0] / file_length)\n",
    "\n",
    "print(frames_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d26ff48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64883465\n"
     ]
    }
   ],
   "source": [
    "# Example tensors (replace with actual data)\n",
    "y_true = tf.constant(onsets)  # Ground truth binary representation\n",
    "y_prediction = tf.constant(y_pred[idx]['onset'])  # Model's predicted probabilities\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "loss = bce(y_true, y_prediction)\n",
    "print(loss.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c37ffa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# Initialize the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ab33b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Loop through batches\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            logits = model(x_batch, training=True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss_value = loss_fn(y_batch, logits)\n",
    "        \n",
    "        # Get gradients\n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "        \n",
    "        # Update the metric\n",
    "        metric.update_state(y_batch, logits)\n",
    "        \n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}: loss = {loss_value:.4f}, accuracy = {metric.result().numpy():.4f}\")\n",
    "            \n",
    "    # Reset metric at the end of epoch\n",
    "    metric.reset_states()\n",
    "    val_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        val_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "    print(f\"Validation accuracy: {val_metric.result().numpy():.4f}\")\n",
    "    val_metric.reset_states()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
