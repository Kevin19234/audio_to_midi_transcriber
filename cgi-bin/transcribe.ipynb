{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cgi\n",
    "import os\n",
    "import json\n",
    "from tensorflow import saved_model\n",
    "from basic_pitch import inference\n",
    "\n",
    "# Enable debugging\n",
    "import cgitb\n",
    "cgitb.enable()\n",
    "\n",
    "# set paths\n",
    "TEMP_FILE_STORAGE_PATH = '../temp_file_storage/'\n",
    "MODEL_STORAGE_PATH = '../usable_models/'\n",
    "cwd = os.getcwd()\n",
    "\n",
    "def print_response(status, data):\n",
    "    json_response = {'status': status, 'data': data}\n",
    "    print(json.dumps(json_response))\n",
    "\n",
    "audio_file = TEMP_FILE_STORAGE_PATH + 'Wii Music.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'audio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\xampp\\htdocs\\FALL2023_IndependentStudy\\audio_to_midi_vst\\cgi-bin\\transcribe.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m form \u001b[39m=\u001b[39m cgi\u001b[39m.\u001b[39mFieldStorage()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Get the file field\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m fileitem \u001b[39m=\u001b[39m form[\u001b[39m'\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m selected_model_name \u001b[39m=\u001b[39m form\u001b[39m.\u001b[39mgetvalue(\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Test if the file was uploaded\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\MusicTranscriberClean\\lib\\cgi.py:525\u001b[0m, in \u001b[0;36mFieldStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    523\u001b[0m     \u001b[39mif\u001b[39;00m item\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m key: found\u001b[39m.\u001b[39mappend(item)\n\u001b[0;32m    524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m found:\n\u001b[1;32m--> 525\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m    526\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(found) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    527\u001b[0m     \u001b[39mreturn\u001b[39;00m found[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'audio'"
     ]
    }
   ],
   "source": [
    "# Create instance of FieldStorage\n",
    "form = cgi.FieldStorage()\n",
    "\n",
    "# Get the file field\n",
    "fileitem = form['audio']\n",
    "selected_model_name = form.getvalue('model', None)\n",
    "\n",
    "# Test if the file was uploaded\n",
    "if fileitem.filename:\n",
    "    # strip leading path from file name to avoid directory traversal attacks\n",
    "    fn = os.path.basename(fileitem.filename.replace(\"\\\\\", \"/\" ))\n",
    "    open(TEMP_FILE_STORAGE_PATH + fn, 'wb').write(fileitem.file.read())\n",
    "else:\n",
    "    print_response('error', 'No file was uploaded')\n",
    "\n",
    "# Test if the model was selected\n",
    "if not selected_model_name or selected_model_name == '':\n",
    "    print_response('error', 'No model was selected')\n",
    "\n",
    "# Get the file\n",
    "audio_file = TEMP_FILE_STORAGE_PATH + fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model path:  ../usable_models/dec04_train_99posweight/\n"
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "selected_model_name = 'our_model'\n",
    "\n",
    "if selected_model_name == 'our_model':\n",
    "    model_path = MODEL_STORAGE_PATH + 'dec04_train_99posweight/'\n",
    "else:\n",
    "    model_path = MODEL_STORAGE_PATH + 'icassp_2022/nmp/'\n",
    "\n",
    "print(\"Selected model path: \", model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../temp_file_storage/Wii Music_basic_pitch.mid\n",
      "\n",
      "Predicting MIDI for ..\\temp_file_storage\\Wii Music.mp3...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "EagerTensor object has no attribute 'reshape'. \n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n      ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\xampp\\htdocs\\FALL2023_IndependentStudy\\audio_to_midi_vst\\cgi-bin\\transcribe.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     os\u001b[39m.\u001b[39mremove(midi_file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Run the inference\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predict_and_save(model_path\u001b[39m=\u001b[39;49mmodel_path, output_directory\u001b[39m=\u001b[39;49mTEMP_FILE_STORAGE_PATH, audio_path_list\u001b[39m=\u001b[39;49m[audio_file], save_midi\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, sonify_midi\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, save_model_outputs\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, save_notes\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Check if midi file exists\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FALL2023_IndependentStudy/audio_to_midi_vst/cgi-bin/transcribe.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(midi_file):\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\MusicTranscriberClean\\lib\\site-packages\\basic_pitch-0.3.0-py3.8.egg\\basic_pitch\\inference.py:441\u001b[0m, in \u001b[0;36mpredict_and_save\u001b[1;34m(audio_path_list, output_directory, save_midi, sonify_midi, save_model_outputs, save_notes, model_path, onset_threshold, frame_threshold, minimum_note_length, minimum_frequency, maximum_frequency, multiple_pitch_bends, melodia_trick, debug_file, sonification_samplerate, midi_tempo)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    440\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\MusicTranscriberClean\\lib\\site-packages\\basic_pitch-0.3.0-py3.8.egg\\basic_pitch\\inference.py:388\u001b[0m, in \u001b[0;36mpredict_and_save\u001b[1;34m(audio_path_list, output_directory, save_midi, sonify_midi, save_model_outputs, save_notes, model_path, onset_threshold, frame_threshold, minimum_note_length, minimum_frequency, maximum_frequency, multiple_pitch_bends, melodia_trick, debug_file, sonification_samplerate, midi_tempo)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    387\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 388\u001b[0m     model_output, midi_data, note_events \u001b[39m=\u001b[39m predict(\n\u001b[0;32m    389\u001b[0m         pathlib\u001b[39m.\u001b[39;49mPath(audio_path),\n\u001b[0;32m    390\u001b[0m         model,\n\u001b[0;32m    391\u001b[0m         onset_threshold,\n\u001b[0;32m    392\u001b[0m         frame_threshold,\n\u001b[0;32m    393\u001b[0m         minimum_note_length,\n\u001b[0;32m    394\u001b[0m         minimum_frequency,\n\u001b[0;32m    395\u001b[0m         maximum_frequency,\n\u001b[0;32m    396\u001b[0m         multiple_pitch_bends,\n\u001b[0;32m    397\u001b[0m         melodia_trick,\n\u001b[0;32m    398\u001b[0m         debug_file,\n\u001b[0;32m    399\u001b[0m         midi_tempo,\n\u001b[0;32m    400\u001b[0m     )\n\u001b[0;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m save_model_outputs:\n\u001b[0;32m    403\u001b[0m         model_output_path \u001b[39m=\u001b[39m build_output_path(audio_path, output_directory, OutputExtensions\u001b[39m.\u001b[39mMODEL_OUTPUT_NPZ)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\MusicTranscriberClean\\lib\\site-packages\\basic_pitch-0.3.0-py3.8.egg\\basic_pitch\\inference.py:303\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(audio_path, model_or_model_path, onset_threshold, frame_threshold, minimum_note_length, minimum_frequency, maximum_frequency, multiple_pitch_bends, melodia_trick, debug_file, midi_tempo)\u001b[0m\n\u001b[0;32m    299\u001b[0m     model \u001b[39m=\u001b[39m model_or_model_path\n\u001b[0;32m    301\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredicting MIDI for \u001b[39m\u001b[39m{\u001b[39;00maudio_path\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 303\u001b[0m model_output \u001b[39m=\u001b[39m run_inference(audio_path, model, debug_file)\n\u001b[0;32m    304\u001b[0m min_note_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mround(minimum_note_length \u001b[39m/\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m (AUDIO_SAMPLE_RATE \u001b[39m/\u001b[39m FFT_HOP)))\n\u001b[0;32m    305\u001b[0m midi_data, note_events \u001b[39m=\u001b[39m infer\u001b[39m.\u001b[39mmodel_output_to_notes(\n\u001b[0;32m    306\u001b[0m     model_output,\n\u001b[0;32m    307\u001b[0m     onset_thresh\u001b[39m=\u001b[39monset_threshold,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     midi_tempo\u001b[39m=\u001b[39mmidi_tempo,\n\u001b[0;32m    315\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\MusicTranscriberClean\\lib\\site-packages\\basic_pitch-0.3.0-py3.8.egg\\basic_pitch\\inference.py:146\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(audio_path, model, debug_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m audio_windowed, _, audio_original_length \u001b[39m=\u001b[39m get_audio_input(audio_path, overlap_len, hop_size)\n\u001b[0;32m    145\u001b[0m output \u001b[39m=\u001b[39m model(audio_windowed)\n\u001b[1;32m--> 146\u001b[0m unwrapped_output \u001b[39m=\u001b[39m {k: unwrap_output(output[k], audio_original_length, n_overlapping_frames) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m output}\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m debug_file:\n\u001b[0;32m    149\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(debug_file, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\MusicTranscriberClean\\lib\\site-packages\\basic_pitch-0.3.0-py3.8.egg\\basic_pitch\\inference.py:146\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    143\u001b[0m audio_windowed, _, audio_original_length \u001b[39m=\u001b[39m get_audio_input(audio_path, overlap_len, hop_size)\n\u001b[0;32m    145\u001b[0m output \u001b[39m=\u001b[39m model(audio_windowed)\n\u001b[1;32m--> 146\u001b[0m unwrapped_output \u001b[39m=\u001b[39m {k: unwrap_output(output[k], audio_original_length, n_overlapping_frames) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m output}\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m debug_file:\n\u001b[0;32m    149\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(debug_file, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\MusicTranscriberClean\\lib\\site-packages\\basic_pitch-0.3.0-py3.8.egg\\basic_pitch\\inference.py:119\u001b[0m, in \u001b[0;36munwrap_output\u001b[1;34m(output, audio_original_length, n_overlapping_frames)\u001b[0m\n\u001b[0;32m    117\u001b[0m output_shape \u001b[39m=\u001b[39m raw_output\u001b[39m.\u001b[39mshape\n\u001b[0;32m    118\u001b[0m n_output_frames_original \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mfloor(audio_original_length \u001b[39m*\u001b[39m (ANNOTATIONS_FPS \u001b[39m/\u001b[39m AUDIO_SAMPLE_RATE)))\n\u001b[1;32m--> 119\u001b[0m unwrapped_output \u001b[39m=\u001b[39m raw_output\u001b[39m.\u001b[39;49mreshape(output_shape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m output_shape[\u001b[39m1\u001b[39m], output_shape[\u001b[39m2\u001b[39m])\n\u001b[0;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m unwrapped_output[:n_output_frames_original, :]\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\MusicTranscriberClean\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:424\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m    421\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mravel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtranspose\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    422\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mtolist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    423\u001b[0m     \u001b[39m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    426\u001b[0m \u001b[39m      If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    427\u001b[0m \u001b[39m      from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    428\u001b[0m \u001b[39m      np_config.enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    429\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m    430\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: EagerTensor object has no attribute 'reshape'. \n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n      "
     ]
    }
   ],
   "source": [
    "from basic_pitch.inference import predict_and_save\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(False)\n",
    "# check if midi file exists\n",
    "midi_file = audio_file.replace('.mp3', '_basic_pitch.mid')\n",
    "print(midi_file)\n",
    "if os.path.exists(midi_file):\n",
    "    os.remove(midi_file)\n",
    "\n",
    "# Run the inference\n",
    "predict_and_save(model_path=model_path, output_directory=TEMP_FILE_STORAGE_PATH, audio_path_list=[audio_file], save_midi=True, sonify_midi=False, save_model_outputs=False, save_notes=False)\n",
    "\n",
    "# Check if midi file exists\n",
    "if os.path.exists(midi_file):\n",
    "    print_response('success', midi_file)\n",
    "else:\n",
    "    print_response('error', 'Midi file was not created')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MusicTranscriber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
